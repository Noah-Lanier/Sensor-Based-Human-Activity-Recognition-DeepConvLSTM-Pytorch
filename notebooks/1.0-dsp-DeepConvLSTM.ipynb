{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file ../data/raw/: Permission denied\n",
      "\n",
      "  0  292M    0 31744    0     0  31744      0  2:40:59 --:--:--  2:40:59 52124\n",
      "curl: (23) Failed writing body (0 != 16384)\n"
     ]
    }
   ],
   "source": [
    "!curl -o ../data/raw/ https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset ../data/raw/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
      "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data/processed\n",
    "!python ../src/data/preprocess_data.py -i  ../data/raw/OpportunityUCIDataset.zip -o ../processed/oppChallenge_gestures.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NB_SENSOR_CHANNELS = 8\n",
    "SLIDING_WINDOW_LENGTH = 20\n",
    "SLIDING_WINDOW_STEP = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from data.sliding_window import sliding_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file ../data/processed/oppChallenge_gestures.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n"
     ]
    }
   ],
   "source": [
    "# def load_dataset(filename):\n",
    "    \n",
    "#     with open(filename, 'rb') as f:\n",
    "#         data = cp.load(f)\n",
    "    \n",
    "#     X_train, y_train = data[0]\n",
    "#     X_test, y_test = data[1]\n",
    "\n",
    "#     print(\" ..from file {}\".format(filename))\n",
    "#     print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "#     X_train = X_train.astype(np.float32)\n",
    "#     X_test = X_test.astype(np.float32)\n",
    "\n",
    "#     # The targets are casted to int8 for GPU compatibility.\n",
    "#     y_train = y_train.astype(np.uint8)\n",
    "#     y_test = y_test.astype(np.uint8)\n",
    "\n",
    "#     return X_train, y_train, X_test, y_test\n",
    "\n",
    "# print(\"Loading data...\")\n",
    "# X_train, y_train, X_test, y_test = load_dataset('../data/processed/oppChallenge_gestures.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DB'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('plain_data_.csv')\n",
    "y = X['Activity']\n",
    "\n",
    "X = X.loc[:,'ACC1':'Magnitude'].to_numpy()\n",
    "X_train = X[:223872]\n",
    "X_test = X[223872:]\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le.fit([\"Activity\", \"Baseline\", \"DB\", \"Type\"])\n",
    "y = le.transform(y)\n",
    "y_train = y[:223872]\n",
    "y_test = y[223872:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11193, 20, 8)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..after sliding and reshaping, train data: inputs (11193, 20, 8), targets (11193,)\n",
      " ..after sliding and reshaping, test data : inputs (2798, 20, 8), targets (2798,)\n"
     ]
    }
   ],
   "source": [
    "assert NB_SENSOR_CHANNELS == X_train.shape[1]\n",
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y, ws, ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "\n",
    "# Data is reshaped\n",
    "X_train = X_train.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)) # for input to Conv1D\n",
    "X_test = X_test.reshape((-1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)) # for input to Conv1D\n",
    "\n",
    "print(\" ..after sliding and reshaping, train data: inputs {0}, targets {1}\".format(X_train.shape, y_train.shape))\n",
    "print(\" ..after sliding and reshaping, test data : inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 41.       ,  27.2      ,  40.       , ...,  15.25     ,\n",
       "          78.98     ,  63.410095 ],\n",
       "        [ 41.       ,  27.3      ,  40.       , ..., -12.75     ,\n",
       "          78.835    ,  63.453053 ],\n",
       "        [ 41.       ,  27.4      ,  40.       , ..., -42.99     ,\n",
       "          78.69     ,  63.496143 ],\n",
       "        ...,\n",
       "        [ 40.4375   ,  28.28125  ,  39.4375   , ..., -25.67     ,\n",
       "          76.2675   ,  63.169113 ],\n",
       "        [ 40.375    ,  28.3125   ,  39.375    , ...,  26.85     ,\n",
       "          76.105    ,  63.10411  ],\n",
       "        [ 40.3125   ,  28.34375  ,  39.3125   , ...,  14.3      ,\n",
       "          75.9425   ,  63.03918  ]],\n",
       "\n",
       "       [[ 40.25     ,  28.375    ,  39.25     , ..., -19.15     ,\n",
       "          75.78     ,  62.974323 ],\n",
       "        [ 40.1875   ,  28.40625  ,  39.1875   , ...,  15.64     ,\n",
       "          75.665    ,  62.909542 ],\n",
       "        [ 40.125    ,  28.4375   ,  39.125    , ...,   7.81     ,\n",
       "          75.55     ,  62.84483  ],\n",
       "        ...,\n",
       "        [ 39.1875   ,  28.90625  ,  38.1875   , ...,  34.83     ,\n",
       "          73.865    ,  61.883087 ],\n",
       "        [ 39.125    ,  28.9375   ,  38.125    , ...,  14.67     ,\n",
       "          73.75     ,  61.819576 ],\n",
       "        [ 39.0625   ,  28.96875  ,  38.0625   , ...,  -6.2      ,\n",
       "          73.635    ,  61.756145 ]],\n",
       "\n",
       "       [[ 39.       ,  29.       ,  38.       , ..., -18.83     ,\n",
       "          73.52     ,  61.692787 ],\n",
       "        [ 39.065216 ,  28.934782 ,  38.02174  , ...,  -0.3      ,\n",
       "          73.435    ,  61.716816 ],\n",
       "        [ 39.130436 ,  28.869566 ,  38.04348  , ...,  11.03     ,\n",
       "          73.35     ,  61.740982 ],\n",
       "        ...,\n",
       "        [ 40.108696 ,  27.891304 ,  38.369564 , ...,   2.44     ,\n",
       "          72.0425   ,  62.11969  ],\n",
       "        [ 40.173912 ,  27.826086 ,  38.391304 , ...,  14.93     ,\n",
       "          71.955    ,  62.14601  ],\n",
       "        [ 40.239132 ,  27.76087  ,  38.413044 , ...,  10.01     ,\n",
       "          71.8675   ,  62.172466 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 52.917603 ,  -2.5730338,  21.962547 , ...,  21.46     ,\n",
       "          69.22     ,  57.351955 ],\n",
       "        [ 52.88015  ,  -2.6516855,  22.12734  , ...,  14.92     ,\n",
       "          69.1775   ,  57.384327 ],\n",
       "        [ 52.842697 ,  -2.7303371,  22.292135 , ...,  -9.53     ,\n",
       "          69.135    ,  57.417286 ],\n",
       "        ...,\n",
       "        [ 52.2809   ,  -3.9101124,  24.764046 , ...,  -9.98     ,\n",
       "          68.4425   ,  57.98137  ],\n",
       "        [ 52.243446 ,  -3.988764 ,  24.928839 , ..., -29.44     ,\n",
       "          68.405    ,  58.02357  ],\n",
       "        [ 52.205994 ,  -4.0674157,  25.093634 , ...,  19.27     ,\n",
       "          68.3675   ,  58.06634  ]],\n",
       "\n",
       "       [[ 52.16854  ,  -4.1460676,  25.258427 , ...,  25.92     ,\n",
       "          68.33     ,  58.109676 ],\n",
       "        [ 52.131084 ,  -4.224719 ,  25.423222 , ..., -12.35     ,\n",
       "          68.2975   ,  58.153576 ],\n",
       "        [ 52.09363  ,  -4.303371 ,  25.588015 , ..., -18.31     ,\n",
       "          68.265    ,  58.198044 ],\n",
       "        ...,\n",
       "        [ 51.531834 ,  -5.483146 ,  28.059925 , ...,  -4.26     ,\n",
       "          67.7225   ,  58.931778 ],\n",
       "        [ 51.49438  ,  -5.5617976,  28.22472  , ..., -16.66     ,\n",
       "          67.675    ,  58.98508  ],\n",
       "        [ 51.45693  ,  -5.6404495,  28.389513 , ...,  19.81     ,\n",
       "          67.6275   ,  59.038925 ]],\n",
       "\n",
       "       [[ 51.419476 ,  -5.719101 ,  28.554308 , ...,   8.52     ,\n",
       "          67.58     ,  59.093307 ],\n",
       "        [ 51.382023 ,  -5.797753 ,  28.7191   , ...,   0.5      ,\n",
       "          67.535    ,  59.148228 ],\n",
       "        [ 51.34457  ,  -5.8764043,  28.883896 , ..., -17.16     ,\n",
       "          67.49     ,  59.203686 ],\n",
       "        ...,\n",
       "        [ 50.782772 ,  -7.05618  ,  31.355804 , ...,   2.09     ,\n",
       "          66.8675   ,  60.098804 ],\n",
       "        [ 50.74532  ,  -7.1348314,  31.5206   , ...,  15.16     ,\n",
       "          66.835    ,  60.162624 ],\n",
       "        [ 50.707867 ,  -7.2134833,  31.685392 , ...,  -1.33     ,\n",
       "          66.8025   ,  60.226955 ]]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HARModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=128, n_layers=1, n_filters=64, \n",
    "                 n_classes=4, filter_size=5, drop_prob=0.5):\n",
    "        super(HARModel, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_filters = n_filters\n",
    "        self.n_classes = n_classes\n",
    "        self.filter_size = filter_size\n",
    "             \n",
    "        self.conv1 = nn.Conv1d(NB_SENSOR_CHANNELS, n_filters, filter_size)\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv3 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        self.conv4 = nn.Conv1d(n_filters, n_filters, filter_size)\n",
    "        \n",
    "        self.lstm1  = nn.LSTM(n_filters, n_hidden, n_layers)\n",
    "        self.lstm2  = nn.LSTM(n_hidden, n_hidden, n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "        \n",
    "        x = x.reshape(-1, NB_SENSOR_CHANNELS, SLIDING_WINDOW_LENGTH)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(8, -1, self.n_filters)\n",
    "        x, hidden = self.lstm1(x, hidden)\n",
    "        x, hidden = self.lstm2(x, hidden)\n",
    "        \n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        out = x.view(batch_size, -1, self.n_classes)[:,-1,:]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "net = HARModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HARModel(\n",
       "  (conv1): Conv1d(8, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv2): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv3): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
       "  (lstm1): LSTM(64, 20)\n",
       "  (lstm2): LSTM(20, 20)\n",
       "  (fc): Linear(in_features=20, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Conv1d or type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "net.apply(init_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=True):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "## check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (1, 50, 20), got [1, 100, 20]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-9c574dc76081>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \"F1-Score: {:.4f}...\".format(f1score/(len(X_test)//batch_size)))\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-138-9c574dc76081>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, epochs, batch_size, lr)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# get the output from the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lpne\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-1a13edc7d365>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, hidden, batch_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lpne\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lpne\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lpne\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0m\u001b[0;32m    534\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[0;32m    535\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\lpne\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    194\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 50, 20), got [1, 100, 20]"
     ]
    }
   ],
   "source": [
    "def train(net, epochs=10, batch_size=100, lr=0.01):\n",
    "    \n",
    "    opt = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "     \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)         \n",
    "        train_losses = []    \n",
    "        net.train()\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "            x, y = batch\n",
    "\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            opt.zero_grad()   \n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h, batch_size)\n",
    "            \n",
    "            loss = criterion(output, targets.long())\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        val_h = net.init_hidden(batch_size)\n",
    "        val_losses = []\n",
    "        accuracy=0\n",
    "        f1score=0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "                x, y = batch     \n",
    "\n",
    "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                output, val_h= net(inputs, val_h, batch_size)\n",
    "\n",
    "                val_loss = criterion(output, targets.long())\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                top_p, top_class = output.topk(1, dim=1)\n",
    "                equals = top_class == targets.view(*top_class.shape).long()\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='weighted')\n",
    "            \n",
    "        net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        \"Train Loss: {:.4f}...\".format(np.mean(train_losses)),\n",
    "        \"Val Loss: {:.4f}...\".format(np.mean(val_losses)),\n",
    "        \"Val Acc: {:.4f}...\".format(accuracy/(len(X_test)//batch_size)),\n",
    "        \"F1-Score: {:.4f}...\".format(f1score/(len(X_test)//batch_size)))\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
